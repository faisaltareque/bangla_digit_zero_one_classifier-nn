{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ef082e-d5bb-46fa-9182-f26fbab2f1b8",
   "metadata": {},
   "source": [
    "### All Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bff974-ce32-447f-8894-1d52f2c935dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ac9f96-4342-4759-b22e-5635985e6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    digit_direcroty_image_list = []\n",
    "    for i in range(2):\n",
    "        mypath = os.path.join('BanglaDigitZeroOne', str(i))\n",
    "        onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "        digit_direcroty_image_list.append([i, mypath, onlyfiles])\n",
    "\n",
    "    image_digit_list = []\n",
    "    for item in digit_direcroty_image_list:\n",
    "        digit = item[0]\n",
    "        digit_directory = item[1]\n",
    "        image_list = item[2]\n",
    "        for image in image_list:\n",
    "            path = os.path.join(digit_directory, image)\n",
    "            img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "            resized = cv2.resize(img, (64,64), interpolation = cv2.INTER_AREA)\n",
    "            image_digit_list.append([resized, digit])\n",
    "    random.shuffle(image_digit_list)\n",
    "    X = [i[0] for i in image_digit_list]  # Image 64*64 ndarray to X\n",
    "    y = [i[1] for i in image_digit_list]  # Image label to y\n",
    "    X = np.array(X)                       # ndarray list to numpy array shape (3964, 64, 64)\n",
    "    y = np.array(y)                       # list to numpy array unranked shape (3964,)\n",
    "    y = y.reshape(1,y.shape[0])           # unranked array to (1, 3964) shaped array\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e53498-a630-4d01-b855-34089e4b7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba3ee60-a2c6-4b1d-b34f-54396e9d70da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3964, 64, 64)\n",
      "(1, 3964)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)                       # (3964, 64, 64)\n",
    "print(y.shape)                       # (1, 3964)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0818fd95-409f-4691-b9da-05414b2d0bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 546\n",
    "print(np.squeeze(y[:, index]))\n",
    "plt.imshow(X[index], cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f402eb-3a5c-4248-bcb4-207739cab52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_length = len(X)\n",
    "train_length = math.ceil(dataset_length*.8)\n",
    "train_set_x_orig = X[:train_length]          # (3172, 64, 64)\n",
    "train_set_y = y[:,:train_length]             # (1, 3172)\n",
    "test_set_x_orig =X[train_length:]            # (792, 64, 64)\n",
    "test_set_y = y[:,train_length:]              # (1, 792)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44f53126-51b7-4f81-b232-114f78ba0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[1]*train_set_x_orig.shape[2],train_set_x_orig.shape[0])     # (3172, 64, 64) ==> (4096, 3172)\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[1]*test_set_x_orig.shape[2],test_set_x_orig.shape[0])          # (792, 64, 64) ==> (4096, 792)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f928fca-3867-4a16-aea4-a084b4761eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (4096, 3172)\n",
      "train_set_y shape: (1, 3172)\n",
      "test_set_x_flatten shape: (4096, 792)\n",
      "test_set_y shape: (1, 792)\n"
     ]
    }
   ],
   "source": [
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37abf28a-2713-470c-be28-7f1d83b77fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb3be6b2-a434-456b-a5df-45eccbe78e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (4096, 3172)\n",
      "The shape of Y is: (1, 3172)\n",
      "I have m = 3172 training examples!\n"
     ]
    }
   ],
   "source": [
    "shape_X = train_set_x.shape\n",
    "shape_Y = train_set_y.shape\n",
    "m = train_set_x.shape[1]\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62bb1f-7f42-48d8-9112-cffc96781033",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31e04d-2f3c-450f-aff6-2122bc10e973",
   "metadata": {},
   "source": [
    "### Defining the neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b045e773-d5f5-4038-a53e-63665a0b3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    n_h = 4\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6184e4-e389-4cdf-a131-bebda37e0d9e",
   "metadata": {},
   "source": [
    "### Initialize the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71275143-0e58-4059-9506-e37637658aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) \n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac0a31-9899-46c9-8255-b3c133b7e7fc",
   "metadata": {},
   "source": [
    "### Forward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9daefc9-cad5-4b18-bf14-87e1d6dc103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6971801-eff4-4ed0-bb16-e08ceb055821",
   "metadata": {},
   "source": [
    "### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "235de4cc-0574-43cb-9882-969ba2d71c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    [Note that the parameters argument is not used in this function, \n",
    "    but the auto-grader currently expects this parameter.\n",
    "    Future version of this notebook will fix both the notebook \n",
    "    and the auto-grader so that `parameters` is not needed.\n",
    "    For now, please include `parameters` in the function signature,\n",
    "    and also when invoking this function.]\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    logprobs = np.multiply(np.log(A2),Y)+np.multiply((np.log(1-A2)),(1-Y))\n",
    "    cost = (-1/m)*np.sum(logprobs)\n",
    "\n",
    "    \n",
    "    cost = float(np.squeeze(cost))\n",
    "                                    \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c95403-41d2-4005-85fa-4946017fb444",
   "metadata": {},
   "source": [
    "### Backward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7bcb82f-8925-4e70-a210-9fef50c66af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m)*(np.dot(dZ2,A1.T))\n",
    "    db2 = (1/m)*np.sum(dZ2, axis = 1, keepdims =True)\n",
    "    dZ1 = np.dot(W2.T, dZ2)*(1-np.power(A1,2))\n",
    "    dW1 = (1/m)*(np.dot(dZ1,X.T))\n",
    "    db1 = (1/m)*np.sum(dZ1, axis = 1, keepdims =True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc51c1-cd41-4cc4-bb3e-c8d497755254",
   "metadata": {},
   "source": [
    "### Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "590d0e4d-50ee-420c-92f0-4e49c04a053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370f424-6f56-455f-a56e-f24199f02543",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "427eb8ea-2133-4a6d-8820-8f0ebb10eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_model\n",
    "def nn_model(X, Y, n_h, learning_rate, num_iterations = 10000, print_cost=False):\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\"\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\"\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\"\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        # Update rule for each parameter\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        # If print_cost=True, Print the cost every 1000 iterations\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # Returns parameters learnt by the model. They can then be used to predict output\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc6a964c-8c1e-4fa7-8c77-1a63b83997b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e8d55-81c7-4d25-9092-1e5488a66e55",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f1e1e02-eb24-4756-bc01-ec33c4053770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae60c91-9f77-4311-92a4-2fdf42fdaf90",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6bcea49-1516-44f7-bb9a-406884e832b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693174\n",
      "Cost after iteration 100: 0.670671\n",
      "Cost after iteration 200: 0.657527\n",
      "Cost after iteration 300: 0.635860\n",
      "Cost after iteration 400: 0.661519\n",
      "Cost after iteration 500: 0.605139\n",
      "Cost after iteration 600: 0.659054\n",
      "Cost after iteration 700: 0.680658\n",
      "Cost after iteration 800: 0.719673\n",
      "Cost after iteration 900: 0.685962\n",
      "Cost after iteration 1000: 0.603412\n",
      "Cost after iteration 1100: 0.596689\n",
      "Cost after iteration 1200: 0.548795\n",
      "Cost after iteration 1300: 0.563464\n",
      "Cost after iteration 1400: 0.589407\n",
      "Cost after iteration 1500: 0.568772\n",
      "Cost after iteration 1600: 0.483844\n",
      "Cost after iteration 1700: 0.596430\n",
      "Cost after iteration 1800: 0.654420\n",
      "Cost after iteration 1900: 0.452719\n",
      "Cost after iteration 2000: 0.534709\n",
      "Cost after iteration 2100: 0.550877\n",
      "Cost after iteration 2200: 0.523490\n",
      "Cost after iteration 2300: 0.421931\n",
      "Cost after iteration 2400: 0.467539\n",
      "Cost after iteration 2500: 0.528212\n",
      "Cost after iteration 2600: 0.606649\n",
      "Cost after iteration 2700: 0.570131\n",
      "Cost after iteration 2800: 0.562355\n",
      "Cost after iteration 2900: 0.405416\n",
      "Cost after iteration 3000: 0.419064\n",
      "Cost after iteration 3100: 0.448647\n",
      "Cost after iteration 3200: 0.361462\n",
      "Cost after iteration 3300: 0.387240\n",
      "Cost after iteration 3400: 0.526484\n",
      "Cost after iteration 3500: 0.452613\n",
      "Cost after iteration 3600: 0.382641\n",
      "Cost after iteration 3700: 0.864847\n",
      "Cost after iteration 3800: 0.473494\n",
      "Cost after iteration 3900: 0.325941\n",
      "Cost after iteration 4000: 0.521568\n",
      "Cost after iteration 4100: 0.412453\n",
      "Cost after iteration 4200: 0.494568\n",
      "Cost after iteration 4300: 0.404717\n",
      "Cost after iteration 4400: 0.332500\n",
      "Cost after iteration 4500: 0.494040\n",
      "Cost after iteration 4600: 0.335583\n",
      "Cost after iteration 4700: 0.348023\n",
      "Cost after iteration 4800: 0.512346\n",
      "Cost after iteration 4900: 0.375058\n",
      "Cost after iteration 5000: 0.442586\n",
      "Cost after iteration 5100: 0.413577\n",
      "Cost after iteration 5200: 0.692427\n",
      "Cost after iteration 5300: 0.358226\n",
      "Cost after iteration 5400: 0.328882\n",
      "Cost after iteration 5500: 0.436271\n",
      "Cost after iteration 5600: 0.345622\n",
      "Cost after iteration 5700: 0.458003\n",
      "Cost after iteration 5800: 0.652760\n",
      "Cost after iteration 5900: 0.357737\n",
      "Cost after iteration 6000: 0.421886\n",
      "Cost after iteration 6100: 0.694685\n",
      "Cost after iteration 6200: 0.402841\n",
      "Cost after iteration 6300: 0.448325\n",
      "Cost after iteration 6400: 0.413760\n",
      "Cost after iteration 6500: 0.296804\n",
      "Cost after iteration 6600: 0.349521\n",
      "Cost after iteration 6700: 0.270779\n",
      "Cost after iteration 6800: 0.444814\n",
      "Cost after iteration 6900: 0.388303\n",
      "Cost after iteration 7000: 0.483087\n",
      "Cost after iteration 7100: 0.468221\n",
      "Cost after iteration 7200: 0.608451\n",
      "Cost after iteration 7300: 0.455724\n",
      "Cost after iteration 7400: 0.299237\n",
      "Cost after iteration 7500: 0.217116\n",
      "Cost after iteration 7600: 0.210642\n",
      "Cost after iteration 7700: 0.271026\n",
      "Cost after iteration 7800: 0.307308\n",
      "Cost after iteration 7900: 0.229829\n",
      "Cost after iteration 8000: 0.398704\n",
      "Cost after iteration 8100: 0.189126\n",
      "Cost after iteration 8200: 0.472483\n",
      "Cost after iteration 8300: 0.382726\n",
      "Cost after iteration 8400: 0.286689\n",
      "Cost after iteration 8500: 0.355482\n",
      "Cost after iteration 8600: 0.558547\n",
      "Cost after iteration 8700: 0.308557\n",
      "Cost after iteration 8800: 0.390070\n",
      "Cost after iteration 8900: 0.235913\n",
      "Cost after iteration 9000: 0.515912\n",
      "Cost after iteration 9100: 0.409516\n",
      "Cost after iteration 9200: 0.693811\n",
      "Cost after iteration 9300: 0.586103\n",
      "Cost after iteration 9400: 0.257364\n",
      "Cost after iteration 9500: 0.226474\n",
      "Cost after iteration 9600: 0.211359\n",
      "Cost after iteration 9700: 0.205980\n",
      "Cost after iteration 9800: 0.200202\n",
      "Cost after iteration 9900: 0.182253\n",
      "train accuracy: 91.2673392181589 %\n"
     ]
    }
   ],
   "source": [
    "# parameters = nn_model(X, Y, 4, 1.2 , num_iterations = 10000, print_cost=True)\n",
    "parameters = nn_model(train_set_x, train_set_y, 4, 0.2 , num_iterations = 10000, print_cost=True)\n",
    "Y_prediction_train = predict(parameters, train_set_x)\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - train_set_y)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c458fbc-af3f-4e5b-8e97-b7f28eff3824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
